\section{Detailed Future Work}
There are various improvements that can be done to improve the M.O.S.I.S UI 2.0, those being:
\begin{enumerate}
	\item Increase the frame rate of the preview by using the Pixelink SDK preview functions instead of doing manual processing on captured images.
          \begin{itemize}
            \item The main limitation with the current implementation is that in order to achieve that the camera previews are embedded within the user interface, images are captured, processed into a usable format and displayed on the user interface. The main crux with this approach is the processing speed of the Raspberry Pi when converting the raw BAYER8 image data into a color space that can be displayed into PyQt6. The processing takes from 1 to 4 times the amount of time than the time it takes to get the raw image data from the camera. The official and better way of getting the camera previews is to use the preview streams from the cameras that each on their separate operating system thread. The main problem with this approach is that the camera SDK does not support embedding the camera feed on Linux which is the operating system that the Raspberry Pi runs. This results in having floating windows when a camera preview starts. The user interface would have to be redesigned to accommodate or manipulate the floating windows while also maintaining high visibility under the water. This can be done by recycling a large portion of the back end logic with the buttons, cameras, lighting, database and folder structure generator and redesign the user interface to accommodate the floating camera preview windows. This should be done with care since this implementation takes into account the visibility of the user interface under water and thus any future implementation should do the same.
          \end{itemize}
	\item Display button controls for each menu.
          \begin{itemize}
            \item This would drastically decrease the learning curve of using the device because due to how limited the control scheme is on this device, each menu has a distinct control scheme that uses the on board buttons in slightly different ways. Therefore, labeling the inputs depending on which menu is active would drastically increase usability. This can be done by either hard codding the labels within each one of the menus or by creating an on screen display API that overlays the controls on the display without modifying the underlying source code of the menus.
          \end{itemize}
	\item Have the onboard buttons have hold functionality.
          \begin{itemize}
            \item The current implementation of the buttons are based on interrupts. Meaning that an input is only registered once when an input even when an input is held. This can be implemented within the GPIO decoder where while an input is detected, keep emitting the key press signal.
          \end{itemize}
	\item Implement PWM for on-board lighting.
          \begin{itemize}
            \item Since the Raspberry Pi only has 1 PWM, software PWM has to be implemented using operating system threads as to avoid blocking the main thread.
          \end{itemize}
	\item Implement focus step manipulation using the on-board buttons.
	\item Implement auto focus for the cameras.
	\item Focus point visualizer.
	\item Visible preview when adjusting camera settings.
	\item Standardize gain to use ISO measurements.
	\item Swap left and right camera to reflect stereoscopic capture.
	\item Lighting toggle on preview screen.
\end{enumerate}
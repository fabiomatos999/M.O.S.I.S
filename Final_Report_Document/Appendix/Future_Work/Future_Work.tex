\section{Detailed Future Work}
There are various improvements that can be done to improve the M.O.S.I.S UI 2.0, those being:
\begin{enumerate}
	\item Increase the frame rate of the preview by using the Pixelink SDK preview functions instead of doing manual processing on captured images.
          \begin{itemize}
            \item The main limitation with the current implementation is that in order to achieve that the camera previews are embedded within the user interface, images are captured, processed into a usable format and displayed on the user interface. The main crux with this approach is the processing speed of the Raspberry Pi when converting the raw BAYER8 image data into a color space that can be displayed into PyQt6. The processing takes from 1 to 4 times the amount of time than the time it takes to get the raw image data from the camera. The official and better way of getting the camera previews is to use the preview streams from the cameras that each on their separate operating system thread. The main problem with this approach is that the camera SDK does not support embedding the camera feed on Linux which is the operating system that the Raspberry Pi runs. This results in having floating windows when a camera preview starts. The user interface would have to be redesigned to accommodate or manipulate the floating windows while also maintaining high visibility under the water. This can be done by recycling a large portion of the back end logic with the buttons, cameras, lighting, database and folder structure generator and redesign the user interface to accommodate the floating camera preview windows. This should be done with care since this implementation takes into account the visibility of the user interface under water and thus any future implementation should do the same.
          \end{itemize}
	\item Display button controls for each menu.
          \begin{itemize}
            \item This would drastically decrease the learning curve of using the device because due to how limited the control scheme is on this device, each menu has a distinct control scheme that uses the on board buttons in slightly different ways. Therefore, labeling the inputs depending on which menu is active would drastically increase usability. This can be done by either hard codding the labels within each one of the menus or by creating an on screen display API that overlays the controls on the display without modifying the underlying source code of the menus.
          \end{itemize}
	\item Have the onboard buttons have hold functionality.
          \begin{itemize}
            \item The current implementation of the buttons are based on interrupts. Meaning that an input is only registered once when an input even when an input is held. This can be implemented within the GPIO decoder where while an input is detected, keep emitting the key press signal.
          \end{itemize}
	\item Implement PWM for on-board lighting.
          \begin{itemize}
            \item Since the Raspberry Pi only has 1 PWM, software PWM has to be implemented using operating system threads as to avoid blocking the main thread. Another way to accomplish this is to use a low power micro controller like an MSP430 to use the available PWM peripherals to control brightness and to communicate with the Raspberry Pi, use either the SPI or $i^2$ interface on the Pi to control with type of lighting to be used and at what brightness.
          \end{itemize}
	\item Implement focus step manipulation using the on-board buttons.
          \begin{itemize}
            \item The current implementation has a step size of 5 focus steps each time the change focus buttons are pressed. This has a high level of granularity but the ability to quickly move between the entire focus range is hampered substantially. Therefore using a button to cycle between various different focus step increments not only provides the ability to select a focus step that allows to quickly go through the entire focus range but can also be used to increase the granularity to focus steps like 1 or 0.5.
          \end{itemize}
	\item Implement auto focus for the cameras.
          \begin{itemize}
           \item  The previous implementation of the user interface has a working auto focus. However, due to a combination of poor documentation of the camera SDK and poor documentation of the first user interface auto focus was not able to be implemented in time for the final demonstration. Having this feature implemented into the new user interface would greatly decrease the need to manual focus as is necessary with the current implementation. Note however, as far as was discovered during the implementation of the user interface, the Pixelink SDK does not return the current focus value when it is auto focus mode, thus implementation a relative manual focus feature is as we know, impossible unless it is manually implemented using the sharpness score of the cameras.
          \end{itemize}
	\item Focus point visualizer.
          \begin{itemize}
            \item The current implementation allows the user to use the entire focus range of the camera from 1 to 46,000. However there is no way for the user to know at what point in this range is the focus currently at. Having a visualization of the focus point would increase the usability of the M.O.S.I.S microscope.
          \end{itemize}
	\item Visible preview when adjusting camera settings.
          \begin{itemize}
            \item The current implementation has controls for all of the common manipulable values of the cameras. In spite of this, the user has to return to the preview screen to the see how the camera settings have changed. Restructuring the user interface to see at the very lease one of the camera previews would drastically decrease the time to set up a good image on the camera, in turn increase usability.
          \end{itemize}
	\item Standardize gain to use ISO measurements.
          \begin{itemize}
            \item Standardizing the gain to use ISO measurements would help in setting the correct exposure when using a light meter.
          \end{itemize}
	\item Swap left and right camera to reflect stereoscopic capture.
          \begin{itemize}
            \item The initial thought during implementation was that the camera previews be relative to how the operator sees the cameras. However, to achieve stereoscopic vision, the left and right cameras have to be swapped due to how they are mounted inside the chassis. Changing this behavior is simply to swap the order of the camera handles when the cameras are initialized.
          \end{itemize}
	\item Lighting toggle on preview screen.
          \begin{itemize}
            \item Having the ability to cycle through the onboard lighting would allow the operator to see which lighting would the subject look best using and thus increase usability of the device. This can be done relatively easily since there is an unused button on the preview screen.
          \end{itemize}
\end{enumerate}
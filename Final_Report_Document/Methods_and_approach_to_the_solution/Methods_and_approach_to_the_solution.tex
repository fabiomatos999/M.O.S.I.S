\section{Methods and approach to the solution (Fabio)}
The team for this project comprised of two individuals: Fabio J. Matos Nieves and Eduardo S. Miranda Figueroa. Initially both of were tasked with leaning about PyQt6 via a course on Udemy. Once both of us finished the relevant parts of the course, then we started creating the creating the different menus using the mock-ups and system architecture as a base. Fabio was tasked with creating the preview screen and study profile screen markups while Eduardo worked on the sensor and camera control menus, a total of 8 between the both of us. Once the markup was drafted, then we started work on connecting the different menu elements to the selection labels where applicable. Once that was completed, we set off to work on different modules, Fabio started work on the hardware API where Eduardo worked on the back end and folder structure generator. The back end and folder structure generator were completed on schedule without any delay. However the hardware API took an additional, who's original deadline was the fifteenth of November took an additional 9 days to complete. This is due in part to many factors, mainly that, in hindsight, the hardware API in large part was the integration of the hardware and the software. This meant that in order demonstrate the module for the second happy hour section, a large portion of the hardware had to be integrated, thus taking development time from actually finishing the hardware API. Another reason or the delay was the difficulty with increasing the frames per second of the camera previews. The SDK for the cameras used on the M.O.S.I.S microscope claimed that the camera feeds can be embedded within the application and they did provide an example in PyQt5, however the documentation forgot to mention that embedding the camera previews are a Microsoft Window's exclusive feature and thus embedding the camera previews would be impossible. The preview function attempted to be optimized, however, the bottleneck was found within the conversion of the BAYER8 raw sensor data and a usable RGB image, wherein it would take on average 1 to 5 times longer to convert an image to RGB than to actually receive the sensor data. Another reason for the delay was the integration with the on board buttons, where the schematics of the buttons had to be located since they were missing from the existing project files for the M.O.S.I.S microscope and there were unforeseen problems with using the arrow keys as the bindings for the on board buttons. They would function properly when the user interface was controlled by using the arrow keys on a computer but would not do the same when the arrow key signals were emitted inside of the user interface. We initially had 12 days of leeway between the completion of the hardware API, which also encompassed integration, thus we used these 12 days to complete the user interface integration with the cameras and the on board buttons. As for how the system specifications were validated, we preliminary went through each one of the amended requirements, like a checklist, and checked if the implemented and integrated features fulfilled with the requirements from the client. The system was tested, for the features that manipulate the cameras, we verified that whatever changes were made to the image, i.e, shutter speed, gain, white balance and saturation were reflected in both the preview image and the saved images. For the features that were related to the execution of the study profile, around 200 sample images were taken to verify that the cameras captured the images correctly and with the correct parameters of the study profile, including lighting settings. Finally for the buttons, these were tested by extensively using them during development in order to test other features in the user interface.